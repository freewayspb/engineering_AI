services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    # Оптимизация производительности Ollama:
    # - OLLAMA_KEEP_ALIVE: время хранения модели в памяти (24h = не выгружать)
    # - OLLAMA_NUM_PARALLEL: количество параллельных запросов
    # - OLLAMA_MAX_LOADED_MODELS: максимум моделей в памяти одновременно
    environment:
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU:-1}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-35}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_LOADED_MODELS=2
    runtime: nvidia
    shm_size: '2gb'  # Shared memory для больших моделей (важно для производительности)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Использовать все доступные GPU
              capabilities: [gpu]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ba-ai-gost-backend
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - API_HOST=0.0.0.0
      - API_PORT=8080
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8080:8080"
    restart: unless-stopped

  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      - ollama
    environment:
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU:-1}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-35}
      - OLLAMA_HOST=http://ollama:11434
      - BASE_MODEL=${BASE_MODEL:-llama3.1:8b}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    runtime: nvidia
    shm_size: '2gb'  # Shared memory для больших моделей
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Использовать все доступные GPU
              capabilities: [gpu]
    entrypoint: ["/bin/bash", "-lc", "BASE_MODEL=$BASE_MODEL bash /opt/ollama/scripts/create_models.sh"]
    restart: "no"

volumes:
  ollama:


