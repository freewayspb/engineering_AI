version: "3.9"
services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    restart: unless-stopped

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ba-ai-gost-backend
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - API_HOST=0.0.0.0
      - API_PORT=8080
    depends_on:
      - ollama
    ports:
      - "8080:8080"
    restart: unless-stopped

  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      - ollama
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - BASE_MODEL=${BASE_MODEL:-llama3.1:8b}
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    entrypoint: ["/bin/bash", "-lc", "chmod +x /opt/ollama/scripts/create_models.sh && BASE_MODEL=$BASE_MODEL bash /opt/ollama/scripts/create_models.sh"]
    restart: "no"

volumes:
  ollama:


