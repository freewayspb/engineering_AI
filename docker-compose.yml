services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    environment:
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU:-1}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-35}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
    runtime: nvidia
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 10s
      timeout: 5s
      retries: 10

  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: ba-ai-gost-backend
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - API_HOST=0.0.0.0
      - API_PORT=8080
    depends_on:
      ollama:
        condition: service_healthy
    ports:
      - "8080:8080"
    restart: unless-stopped

  ollama-init:
    image: ollama/ollama:latest
    container_name: ollama-init
    depends_on:
      - ollama
    environment:
      - OLLAMA_NUM_GPU=${OLLAMA_NUM_GPU:-1}
      - OLLAMA_GPU_LAYERS=${OLLAMA_GPU_LAYERS:-35}
      - OLLAMA_HOST=http://ollama:11434
      - BASE_MODEL=${BASE_MODEL:-llama3.1:8b}
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-all}
    volumes:
      - ollama:/root/.ollama
      - ./ollama:/opt/ollama:ro
    runtime: nvidia
    entrypoint: ["/bin/bash", "-lc", "BASE_MODEL=$BASE_MODEL bash /opt/ollama/scripts/create_models.sh"]
    restart: "no"

volumes:
  ollama:


